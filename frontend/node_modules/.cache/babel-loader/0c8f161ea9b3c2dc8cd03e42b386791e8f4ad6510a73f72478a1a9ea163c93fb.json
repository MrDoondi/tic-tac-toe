{"ast":null,"code":"// Machine Learning AI using Q-Learning for Tic-Tac-Toe\nclass MLAI {\n  constructor() {\n    this.qTable = new Map(); // State-action pairs and their Q-values\n    this.learningRate = 0.1;\n    this.discountFactor = 0.9;\n    this.epsilon = 0.1; // For exploration vs exploitation\n    this.trainingMode = false;\n    this.gamesPlayed = 0;\n    this.wins = 0;\n    this.losses = 0;\n    this.draws = 0;\n  }\n\n  // Convert board state to string for storage\n  stateToString(squares) {\n    return squares.map(square => square || '-').join('');\n  }\n\n  // Get all possible actions for a state\n  getAvailableActions(squares) {\n    return squares.map((square, index) => square === null ? index : null).filter(index => index !== null);\n  }\n\n  // Get Q-value for a state-action pair\n  getQValue(state, action) {\n    const key = `${state}-${action}`;\n    return this.qTable.get(key) || 0;\n  }\n\n  // Set Q-value for a state-action pair\n  setQValue(state, action, value) {\n    const key = `${state}-${action}`;\n    this.qTable.set(key, value);\n  }\n\n  // Choose action using epsilon-greedy strategy\n  chooseAction(squares, player) {\n    const state = this.stateToString(squares);\n    const availableActions = this.getAvailableActions(squares);\n    if (availableActions.length === 0) return null;\n\n    // Epsilon-greedy: explore with probability epsilon, exploit with 1-epsilon\n    if (Math.random() < this.epsilon && this.trainingMode) {\n      // Explore: choose random action\n      return availableActions[Math.floor(Math.random() * availableActions.length)];\n    } else {\n      // Exploit: choose best action based on Q-values\n      let bestAction = availableActions[0];\n      let bestQValue = this.getQValue(state, bestAction);\n      for (let action of availableActions) {\n        const qValue = this.getQValue(state, action);\n        if (qValue > bestQValue) {\n          bestQValue = qValue;\n          bestAction = action;\n        }\n      }\n      return bestAction;\n    }\n  }\n\n  // Update Q-values based on reward\n  updateQValue(state, action, reward, nextState, nextAvailableActions) {\n    const currentQ = this.getQValue(state, action);\n\n    // Find maximum Q-value for next state\n    let maxNextQ = 0;\n    if (nextAvailableActions.length > 0) {\n      maxNextQ = Math.max(...nextAvailableActions.map(a => this.getQValue(nextState, a)));\n    }\n\n    // Q-learning update formula\n    const newQ = currentQ + this.learningRate * (reward + this.discountFactor * maxNextQ - currentQ);\n    this.setQValue(state, action, newQ);\n  }\n\n  // Get reward for game outcome\n  getReward(gameResult, player) {\n    if (gameResult === 'draw') return 0.5;\n    if (gameResult === player) return 1.0;\n    return -1.0;\n  }\n\n  // Train the AI through self-play\n  async train(episodes = 10000) {\n    console.log('Starting ML AI training...');\n    this.trainingMode = true;\n    for (let episode = 0; episode < episodes; episode++) {\n      const squares = Array(9).fill(null);\n      const moves = [];\n      let currentPlayer = 'X';\n\n      // Play a complete game\n      while (true) {\n        const state = this.stateToString(squares);\n        const action = this.chooseAction(squares, currentPlayer);\n        if (action === null) break;\n        moves.push({\n          state,\n          action,\n          player: currentPlayer\n        });\n        squares[action] = currentPlayer;\n\n        // Check for game end\n        const winner = this.checkWinner(squares);\n        const isDraw = !winner && this.isBoardFull(squares);\n        if (winner || isDraw) {\n          // Update Q-values for all moves in this game\n          const gameResult = winner || 'draw';\n          for (let i = 0; i < moves.length; i++) {\n            const move = moves[i];\n            const nextState = i < moves.length - 1 ? this.stateToString(squares) : '';\n            const nextActions = i < moves.length - 1 ? this.getAvailableActions(squares) : [];\n            const reward = this.getReward(gameResult, move.player);\n            this.updateQValue(move.state, move.action, reward, nextState, nextActions);\n          }\n\n          // Update statistics\n          this.gamesPlayed++;\n          if (winner === 'X') this.wins++;else if (winner === 'O') this.losses++;else this.draws++;\n          break;\n        }\n        currentPlayer = currentPlayer === 'X' ? 'O' : 'X';\n      }\n\n      // Progress update every 1000 episodes\n      if ((episode + 1) % 1000 === 0) {\n        console.log(`Training progress: ${episode + 1}/${episodes} episodes completed`);\n        console.log(`Win rate: ${(this.wins / this.gamesPlayed * 100).toFixed(1)}%`);\n      }\n    }\n    this.trainingMode = false;\n    console.log('Training completed!');\n    console.log(`Final stats - Games: ${this.gamesPlayed}, Wins: ${this.wins}, Losses: ${this.losses}, Draws: ${this.draws}`);\n  }\n\n  // Make a move using the trained model\n  makeMove(squares, player) {\n    return this.chooseAction(squares, player);\n  }\n\n  // Check for winner (same as minimax)\n  checkWinner(squares) {\n    const lines = [[0, 1, 2], [3, 4, 5], [6, 7, 8],\n    // rows\n    [0, 3, 6], [1, 4, 7], [2, 5, 8],\n    // columns\n    [0, 4, 8], [2, 4, 6] // diagonals\n    ];\n    for (let i = 0; i < lines.length; i++) {\n      const [a, b, c] = lines[i];\n      if (squares[a] && squares[a] === squares[b] && squares[a] === squares[c]) {\n        return squares[a];\n      }\n    }\n    return null;\n  }\n\n  // Check if board is full\n  isBoardFull(squares) {\n    return squares.every(square => square !== null);\n  }\n\n  // Save trained model to localStorage\n  saveModel() {\n    const modelData = {\n      qTable: Object.fromEntries(this.qTable),\n      gamesPlayed: this.gamesPlayed,\n      wins: this.wins,\n      losses: this.losses,\n      draws: this.draws\n    };\n    localStorage.setItem('ticTacToeMLModel', JSON.stringify(modelData));\n    console.log('ML model saved to localStorage');\n  }\n\n  // Load trained model from localStorage\n  loadModel() {\n    const savedData = localStorage.getItem('ticTacToeMLModel');\n    if (savedData) {\n      const modelData = JSON.parse(savedData);\n      this.qTable = new Map(Object.entries(modelData.qTable));\n      this.gamesPlayed = modelData.gamesPlayed || 0;\n      this.wins = modelData.wins || 0;\n      this.losses = modelData.losses || 0;\n      this.draws = modelData.draws || 0;\n      console.log('ML model loaded from localStorage');\n      return true;\n    }\n    return false;\n  }\n\n  // Get training statistics\n  getStats() {\n    return {\n      gamesPlayed: this.gamesPlayed,\n      wins: this.wins,\n      losses: this.losses,\n      draws: this.draws,\n      winRate: this.gamesPlayed > 0 ? (this.wins / this.gamesPlayed * 100).toFixed(1) : 0,\n      modelSize: this.qTable.size\n    };\n  }\n}\nexport default MLAI;","map":{"version":3,"names":["MLAI","constructor","qTable","Map","learningRate","discountFactor","epsilon","trainingMode","gamesPlayed","wins","losses","draws","stateToString","squares","map","square","join","getAvailableActions","index","filter","getQValue","state","action","key","get","setQValue","value","set","chooseAction","player","availableActions","length","Math","random","floor","bestAction","bestQValue","qValue","updateQValue","reward","nextState","nextAvailableActions","currentQ","maxNextQ","max","a","newQ","getReward","gameResult","train","episodes","console","log","episode","Array","fill","moves","currentPlayer","push","winner","checkWinner","isDraw","isBoardFull","i","move","nextActions","toFixed","makeMove","lines","b","c","every","saveModel","modelData","Object","fromEntries","localStorage","setItem","JSON","stringify","loadModel","savedData","getItem","parse","entries","getStats","winRate","modelSize","size"],"sources":["C:/bot/tic-tac-toe/tic-tac-toe/render-deployment/frontend/src/utils/mlAI.js"],"sourcesContent":["// Machine Learning AI using Q-Learning for Tic-Tac-Toe\r\nclass MLAI {\r\n  constructor() {\r\n    this.qTable = new Map(); // State-action pairs and their Q-values\r\n    this.learningRate = 0.1;\r\n    this.discountFactor = 0.9;\r\n    this.epsilon = 0.1; // For exploration vs exploitation\r\n    this.trainingMode = false;\r\n    this.gamesPlayed = 0;\r\n    this.wins = 0;\r\n    this.losses = 0;\r\n    this.draws = 0;\r\n  }\r\n\r\n  // Convert board state to string for storage\r\n  stateToString(squares) {\r\n    return squares.map(square => square || '-').join('');\r\n  }\r\n\r\n  // Get all possible actions for a state\r\n  getAvailableActions(squares) {\r\n    return squares\r\n      .map((square, index) => square === null ? index : null)\r\n      .filter(index => index !== null);\r\n  }\r\n\r\n  // Get Q-value for a state-action pair\r\n  getQValue(state, action) {\r\n    const key = `${state}-${action}`;\r\n    return this.qTable.get(key) || 0;\r\n  }\r\n\r\n  // Set Q-value for a state-action pair\r\n  setQValue(state, action, value) {\r\n    const key = `${state}-${action}`;\r\n    this.qTable.set(key, value);\r\n  }\r\n\r\n  // Choose action using epsilon-greedy strategy\r\n  chooseAction(squares, player) {\r\n    const state = this.stateToString(squares);\r\n    const availableActions = this.getAvailableActions(squares);\r\n    \r\n    if (availableActions.length === 0) return null;\r\n\r\n    // Epsilon-greedy: explore with probability epsilon, exploit with 1-epsilon\r\n    if (Math.random() < this.epsilon && this.trainingMode) {\r\n      // Explore: choose random action\r\n      return availableActions[Math.floor(Math.random() * availableActions.length)];\r\n    } else {\r\n      // Exploit: choose best action based on Q-values\r\n      let bestAction = availableActions[0];\r\n      let bestQValue = this.getQValue(state, bestAction);\r\n\r\n      for (let action of availableActions) {\r\n        const qValue = this.getQValue(state, action);\r\n        if (qValue > bestQValue) {\r\n          bestQValue = qValue;\r\n          bestAction = action;\r\n        }\r\n      }\r\n\r\n      return bestAction;\r\n    }\r\n  }\r\n\r\n  // Update Q-values based on reward\r\n  updateQValue(state, action, reward, nextState, nextAvailableActions) {\r\n    const currentQ = this.getQValue(state, action);\r\n    \r\n    // Find maximum Q-value for next state\r\n    let maxNextQ = 0;\r\n    if (nextAvailableActions.length > 0) {\r\n      maxNextQ = Math.max(...nextAvailableActions.map(a => this.getQValue(nextState, a)));\r\n    }\r\n\r\n    // Q-learning update formula\r\n    const newQ = currentQ + this.learningRate * (reward + this.discountFactor * maxNextQ - currentQ);\r\n    this.setQValue(state, action, newQ);\r\n  }\r\n\r\n  // Get reward for game outcome\r\n  getReward(gameResult, player) {\r\n    if (gameResult === 'draw') return 0.5;\r\n    if (gameResult === player) return 1.0;\r\n    return -1.0;\r\n  }\r\n\r\n  // Train the AI through self-play\r\n  async train(episodes = 10000) {\r\n    console.log('Starting ML AI training...');\r\n    this.trainingMode = true;\r\n    \r\n    for (let episode = 0; episode < episodes; episode++) {\r\n      const squares = Array(9).fill(null);\r\n      const moves = [];\r\n      let currentPlayer = 'X';\r\n      \r\n      // Play a complete game\r\n      while (true) {\r\n        const state = this.stateToString(squares);\r\n        const action = this.chooseAction(squares, currentPlayer);\r\n        \r\n        if (action === null) break;\r\n        \r\n        moves.push({ state, action, player: currentPlayer });\r\n        squares[action] = currentPlayer;\r\n        \r\n        // Check for game end\r\n        const winner = this.checkWinner(squares);\r\n        const isDraw = !winner && this.isBoardFull(squares);\r\n        \r\n        if (winner || isDraw) {\r\n          // Update Q-values for all moves in this game\r\n          const gameResult = winner || 'draw';\r\n          \r\n          for (let i = 0; i < moves.length; i++) {\r\n            const move = moves[i];\r\n            const nextState = i < moves.length - 1 ? this.stateToString(squares) : '';\r\n            const nextActions = i < moves.length - 1 ? this.getAvailableActions(squares) : [];\r\n            const reward = this.getReward(gameResult, move.player);\r\n            \r\n            this.updateQValue(move.state, move.action, reward, nextState, nextActions);\r\n          }\r\n          \r\n          // Update statistics\r\n          this.gamesPlayed++;\r\n          if (winner === 'X') this.wins++;\r\n          else if (winner === 'O') this.losses++;\r\n          else this.draws++;\r\n          \r\n          break;\r\n        }\r\n        \r\n        currentPlayer = currentPlayer === 'X' ? 'O' : 'X';\r\n      }\r\n      \r\n      // Progress update every 1000 episodes\r\n      if ((episode + 1) % 1000 === 0) {\r\n        console.log(`Training progress: ${episode + 1}/${episodes} episodes completed`);\r\n        console.log(`Win rate: ${(this.wins / this.gamesPlayed * 100).toFixed(1)}%`);\r\n      }\r\n    }\r\n    \r\n    this.trainingMode = false;\r\n    console.log('Training completed!');\r\n    console.log(`Final stats - Games: ${this.gamesPlayed}, Wins: ${this.wins}, Losses: ${this.losses}, Draws: ${this.draws}`);\r\n  }\r\n\r\n  // Make a move using the trained model\r\n  makeMove(squares, player) {\r\n    return this.chooseAction(squares, player);\r\n  }\r\n\r\n  // Check for winner (same as minimax)\r\n  checkWinner(squares) {\r\n    const lines = [\r\n      [0, 1, 2], [3, 4, 5], [6, 7, 8], // rows\r\n      [0, 3, 6], [1, 4, 7], [2, 5, 8], // columns\r\n      [0, 4, 8], [2, 4, 6] // diagonals\r\n    ];\r\n\r\n    for (let i = 0; i < lines.length; i++) {\r\n      const [a, b, c] = lines[i];\r\n      if (squares[a] && squares[a] === squares[b] && squares[a] === squares[c]) {\r\n        return squares[a];\r\n      }\r\n    }\r\n    return null;\r\n  }\r\n\r\n  // Check if board is full\r\n  isBoardFull(squares) {\r\n    return squares.every(square => square !== null);\r\n  }\r\n\r\n  // Save trained model to localStorage\r\n  saveModel() {\r\n    const modelData = {\r\n      qTable: Object.fromEntries(this.qTable),\r\n      gamesPlayed: this.gamesPlayed,\r\n      wins: this.wins,\r\n      losses: this.losses,\r\n      draws: this.draws\r\n    };\r\n    localStorage.setItem('ticTacToeMLModel', JSON.stringify(modelData));\r\n    console.log('ML model saved to localStorage');\r\n  }\r\n\r\n  // Load trained model from localStorage\r\n  loadModel() {\r\n    const savedData = localStorage.getItem('ticTacToeMLModel');\r\n    if (savedData) {\r\n      const modelData = JSON.parse(savedData);\r\n      this.qTable = new Map(Object.entries(modelData.qTable));\r\n      this.gamesPlayed = modelData.gamesPlayed || 0;\r\n      this.wins = modelData.wins || 0;\r\n      this.losses = modelData.losses || 0;\r\n      this.draws = modelData.draws || 0;\r\n      console.log('ML model loaded from localStorage');\r\n      return true;\r\n    }\r\n    return false;\r\n  }\r\n\r\n  // Get training statistics\r\n  getStats() {\r\n    return {\r\n      gamesPlayed: this.gamesPlayed,\r\n      wins: this.wins,\r\n      losses: this.losses,\r\n      draws: this.draws,\r\n      winRate: this.gamesPlayed > 0 ? (this.wins / this.gamesPlayed * 100).toFixed(1) : 0,\r\n      modelSize: this.qTable.size\r\n    };\r\n  }\r\n}\r\n\r\nexport default MLAI; "],"mappings":"AAAA;AACA,MAAMA,IAAI,CAAC;EACTC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,MAAM,GAAG,IAAIC,GAAG,CAAC,CAAC,CAAC,CAAC;IACzB,IAAI,CAACC,YAAY,GAAG,GAAG;IACvB,IAAI,CAACC,cAAc,GAAG,GAAG;IACzB,IAAI,CAACC,OAAO,GAAG,GAAG,CAAC,CAAC;IACpB,IAAI,CAACC,YAAY,GAAG,KAAK;IACzB,IAAI,CAACC,WAAW,GAAG,CAAC;IACpB,IAAI,CAACC,IAAI,GAAG,CAAC;IACb,IAAI,CAACC,MAAM,GAAG,CAAC;IACf,IAAI,CAACC,KAAK,GAAG,CAAC;EAChB;;EAEA;EACAC,aAAaA,CAACC,OAAO,EAAE;IACrB,OAAOA,OAAO,CAACC,GAAG,CAACC,MAAM,IAAIA,MAAM,IAAI,GAAG,CAAC,CAACC,IAAI,CAAC,EAAE,CAAC;EACtD;;EAEA;EACAC,mBAAmBA,CAACJ,OAAO,EAAE;IAC3B,OAAOA,OAAO,CACXC,GAAG,CAAC,CAACC,MAAM,EAAEG,KAAK,KAAKH,MAAM,KAAK,IAAI,GAAGG,KAAK,GAAG,IAAI,CAAC,CACtDC,MAAM,CAACD,KAAK,IAAIA,KAAK,KAAK,IAAI,CAAC;EACpC;;EAEA;EACAE,SAASA,CAACC,KAAK,EAAEC,MAAM,EAAE;IACvB,MAAMC,GAAG,GAAG,GAAGF,KAAK,IAAIC,MAAM,EAAE;IAChC,OAAO,IAAI,CAACpB,MAAM,CAACsB,GAAG,CAACD,GAAG,CAAC,IAAI,CAAC;EAClC;;EAEA;EACAE,SAASA,CAACJ,KAAK,EAAEC,MAAM,EAAEI,KAAK,EAAE;IAC9B,MAAMH,GAAG,GAAG,GAAGF,KAAK,IAAIC,MAAM,EAAE;IAChC,IAAI,CAACpB,MAAM,CAACyB,GAAG,CAACJ,GAAG,EAAEG,KAAK,CAAC;EAC7B;;EAEA;EACAE,YAAYA,CAACf,OAAO,EAAEgB,MAAM,EAAE;IAC5B,MAAMR,KAAK,GAAG,IAAI,CAACT,aAAa,CAACC,OAAO,CAAC;IACzC,MAAMiB,gBAAgB,GAAG,IAAI,CAACb,mBAAmB,CAACJ,OAAO,CAAC;IAE1D,IAAIiB,gBAAgB,CAACC,MAAM,KAAK,CAAC,EAAE,OAAO,IAAI;;IAE9C;IACA,IAAIC,IAAI,CAACC,MAAM,CAAC,CAAC,GAAG,IAAI,CAAC3B,OAAO,IAAI,IAAI,CAACC,YAAY,EAAE;MACrD;MACA,OAAOuB,gBAAgB,CAACE,IAAI,CAACE,KAAK,CAACF,IAAI,CAACC,MAAM,CAAC,CAAC,GAAGH,gBAAgB,CAACC,MAAM,CAAC,CAAC;IAC9E,CAAC,MAAM;MACL;MACA,IAAII,UAAU,GAAGL,gBAAgB,CAAC,CAAC,CAAC;MACpC,IAAIM,UAAU,GAAG,IAAI,CAAChB,SAAS,CAACC,KAAK,EAAEc,UAAU,CAAC;MAElD,KAAK,IAAIb,MAAM,IAAIQ,gBAAgB,EAAE;QACnC,MAAMO,MAAM,GAAG,IAAI,CAACjB,SAAS,CAACC,KAAK,EAAEC,MAAM,CAAC;QAC5C,IAAIe,MAAM,GAAGD,UAAU,EAAE;UACvBA,UAAU,GAAGC,MAAM;UACnBF,UAAU,GAAGb,MAAM;QACrB;MACF;MAEA,OAAOa,UAAU;IACnB;EACF;;EAEA;EACAG,YAAYA,CAACjB,KAAK,EAAEC,MAAM,EAAEiB,MAAM,EAAEC,SAAS,EAAEC,oBAAoB,EAAE;IACnE,MAAMC,QAAQ,GAAG,IAAI,CAACtB,SAAS,CAACC,KAAK,EAAEC,MAAM,CAAC;;IAE9C;IACA,IAAIqB,QAAQ,GAAG,CAAC;IAChB,IAAIF,oBAAoB,CAACV,MAAM,GAAG,CAAC,EAAE;MACnCY,QAAQ,GAAGX,IAAI,CAACY,GAAG,CAAC,GAAGH,oBAAoB,CAAC3B,GAAG,CAAC+B,CAAC,IAAI,IAAI,CAACzB,SAAS,CAACoB,SAAS,EAAEK,CAAC,CAAC,CAAC,CAAC;IACrF;;IAEA;IACA,MAAMC,IAAI,GAAGJ,QAAQ,GAAG,IAAI,CAACtC,YAAY,IAAImC,MAAM,GAAG,IAAI,CAAClC,cAAc,GAAGsC,QAAQ,GAAGD,QAAQ,CAAC;IAChG,IAAI,CAACjB,SAAS,CAACJ,KAAK,EAAEC,MAAM,EAAEwB,IAAI,CAAC;EACrC;;EAEA;EACAC,SAASA,CAACC,UAAU,EAAEnB,MAAM,EAAE;IAC5B,IAAImB,UAAU,KAAK,MAAM,EAAE,OAAO,GAAG;IACrC,IAAIA,UAAU,KAAKnB,MAAM,EAAE,OAAO,GAAG;IACrC,OAAO,CAAC,GAAG;EACb;;EAEA;EACA,MAAMoB,KAAKA,CAACC,QAAQ,GAAG,KAAK,EAAE;IAC5BC,OAAO,CAACC,GAAG,CAAC,4BAA4B,CAAC;IACzC,IAAI,CAAC7C,YAAY,GAAG,IAAI;IAExB,KAAK,IAAI8C,OAAO,GAAG,CAAC,EAAEA,OAAO,GAAGH,QAAQ,EAAEG,OAAO,EAAE,EAAE;MACnD,MAAMxC,OAAO,GAAGyC,KAAK,CAAC,CAAC,CAAC,CAACC,IAAI,CAAC,IAAI,CAAC;MACnC,MAAMC,KAAK,GAAG,EAAE;MAChB,IAAIC,aAAa,GAAG,GAAG;;MAEvB;MACA,OAAO,IAAI,EAAE;QACX,MAAMpC,KAAK,GAAG,IAAI,CAACT,aAAa,CAACC,OAAO,CAAC;QACzC,MAAMS,MAAM,GAAG,IAAI,CAACM,YAAY,CAACf,OAAO,EAAE4C,aAAa,CAAC;QAExD,IAAInC,MAAM,KAAK,IAAI,EAAE;QAErBkC,KAAK,CAACE,IAAI,CAAC;UAAErC,KAAK;UAAEC,MAAM;UAAEO,MAAM,EAAE4B;QAAc,CAAC,CAAC;QACpD5C,OAAO,CAACS,MAAM,CAAC,GAAGmC,aAAa;;QAE/B;QACA,MAAME,MAAM,GAAG,IAAI,CAACC,WAAW,CAAC/C,OAAO,CAAC;QACxC,MAAMgD,MAAM,GAAG,CAACF,MAAM,IAAI,IAAI,CAACG,WAAW,CAACjD,OAAO,CAAC;QAEnD,IAAI8C,MAAM,IAAIE,MAAM,EAAE;UACpB;UACA,MAAMb,UAAU,GAAGW,MAAM,IAAI,MAAM;UAEnC,KAAK,IAAII,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGP,KAAK,CAACzB,MAAM,EAAEgC,CAAC,EAAE,EAAE;YACrC,MAAMC,IAAI,GAAGR,KAAK,CAACO,CAAC,CAAC;YACrB,MAAMvB,SAAS,GAAGuB,CAAC,GAAGP,KAAK,CAACzB,MAAM,GAAG,CAAC,GAAG,IAAI,CAACnB,aAAa,CAACC,OAAO,CAAC,GAAG,EAAE;YACzE,MAAMoD,WAAW,GAAGF,CAAC,GAAGP,KAAK,CAACzB,MAAM,GAAG,CAAC,GAAG,IAAI,CAACd,mBAAmB,CAACJ,OAAO,CAAC,GAAG,EAAE;YACjF,MAAM0B,MAAM,GAAG,IAAI,CAACQ,SAAS,CAACC,UAAU,EAAEgB,IAAI,CAACnC,MAAM,CAAC;YAEtD,IAAI,CAACS,YAAY,CAAC0B,IAAI,CAAC3C,KAAK,EAAE2C,IAAI,CAAC1C,MAAM,EAAEiB,MAAM,EAAEC,SAAS,EAAEyB,WAAW,CAAC;UAC5E;;UAEA;UACA,IAAI,CAACzD,WAAW,EAAE;UAClB,IAAImD,MAAM,KAAK,GAAG,EAAE,IAAI,CAAClD,IAAI,EAAE,CAAC,KAC3B,IAAIkD,MAAM,KAAK,GAAG,EAAE,IAAI,CAACjD,MAAM,EAAE,CAAC,KAClC,IAAI,CAACC,KAAK,EAAE;UAEjB;QACF;QAEA8C,aAAa,GAAGA,aAAa,KAAK,GAAG,GAAG,GAAG,GAAG,GAAG;MACnD;;MAEA;MACA,IAAI,CAACJ,OAAO,GAAG,CAAC,IAAI,IAAI,KAAK,CAAC,EAAE;QAC9BF,OAAO,CAACC,GAAG,CAAC,sBAAsBC,OAAO,GAAG,CAAC,IAAIH,QAAQ,qBAAqB,CAAC;QAC/EC,OAAO,CAACC,GAAG,CAAC,aAAa,CAAC,IAAI,CAAC3C,IAAI,GAAG,IAAI,CAACD,WAAW,GAAG,GAAG,EAAE0D,OAAO,CAAC,CAAC,CAAC,GAAG,CAAC;MAC9E;IACF;IAEA,IAAI,CAAC3D,YAAY,GAAG,KAAK;IACzB4C,OAAO,CAACC,GAAG,CAAC,qBAAqB,CAAC;IAClCD,OAAO,CAACC,GAAG,CAAC,wBAAwB,IAAI,CAAC5C,WAAW,WAAW,IAAI,CAACC,IAAI,aAAa,IAAI,CAACC,MAAM,YAAY,IAAI,CAACC,KAAK,EAAE,CAAC;EAC3H;;EAEA;EACAwD,QAAQA,CAACtD,OAAO,EAAEgB,MAAM,EAAE;IACxB,OAAO,IAAI,CAACD,YAAY,CAACf,OAAO,EAAEgB,MAAM,CAAC;EAC3C;;EAEA;EACA+B,WAAWA,CAAC/C,OAAO,EAAE;IACnB,MAAMuD,KAAK,GAAG,CACZ,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;IAAE;IACjC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;IAAE;IACjC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;IAAA,CACtB;IAED,KAAK,IAAIL,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGK,KAAK,CAACrC,MAAM,EAAEgC,CAAC,EAAE,EAAE;MACrC,MAAM,CAAClB,CAAC,EAAEwB,CAAC,EAAEC,CAAC,CAAC,GAAGF,KAAK,CAACL,CAAC,CAAC;MAC1B,IAAIlD,OAAO,CAACgC,CAAC,CAAC,IAAIhC,OAAO,CAACgC,CAAC,CAAC,KAAKhC,OAAO,CAACwD,CAAC,CAAC,IAAIxD,OAAO,CAACgC,CAAC,CAAC,KAAKhC,OAAO,CAACyD,CAAC,CAAC,EAAE;QACxE,OAAOzD,OAAO,CAACgC,CAAC,CAAC;MACnB;IACF;IACA,OAAO,IAAI;EACb;;EAEA;EACAiB,WAAWA,CAACjD,OAAO,EAAE;IACnB,OAAOA,OAAO,CAAC0D,KAAK,CAACxD,MAAM,IAAIA,MAAM,KAAK,IAAI,CAAC;EACjD;;EAEA;EACAyD,SAASA,CAAA,EAAG;IACV,MAAMC,SAAS,GAAG;MAChBvE,MAAM,EAAEwE,MAAM,CAACC,WAAW,CAAC,IAAI,CAACzE,MAAM,CAAC;MACvCM,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BC,IAAI,EAAE,IAAI,CAACA,IAAI;MACfC,MAAM,EAAE,IAAI,CAACA,MAAM;MACnBC,KAAK,EAAE,IAAI,CAACA;IACd,CAAC;IACDiE,YAAY,CAACC,OAAO,CAAC,kBAAkB,EAAEC,IAAI,CAACC,SAAS,CAACN,SAAS,CAAC,CAAC;IACnEtB,OAAO,CAACC,GAAG,CAAC,gCAAgC,CAAC;EAC/C;;EAEA;EACA4B,SAASA,CAAA,EAAG;IACV,MAAMC,SAAS,GAAGL,YAAY,CAACM,OAAO,CAAC,kBAAkB,CAAC;IAC1D,IAAID,SAAS,EAAE;MACb,MAAMR,SAAS,GAAGK,IAAI,CAACK,KAAK,CAACF,SAAS,CAAC;MACvC,IAAI,CAAC/E,MAAM,GAAG,IAAIC,GAAG,CAACuE,MAAM,CAACU,OAAO,CAACX,SAAS,CAACvE,MAAM,CAAC,CAAC;MACvD,IAAI,CAACM,WAAW,GAAGiE,SAAS,CAACjE,WAAW,IAAI,CAAC;MAC7C,IAAI,CAACC,IAAI,GAAGgE,SAAS,CAAChE,IAAI,IAAI,CAAC;MAC/B,IAAI,CAACC,MAAM,GAAG+D,SAAS,CAAC/D,MAAM,IAAI,CAAC;MACnC,IAAI,CAACC,KAAK,GAAG8D,SAAS,CAAC9D,KAAK,IAAI,CAAC;MACjCwC,OAAO,CAACC,GAAG,CAAC,mCAAmC,CAAC;MAChD,OAAO,IAAI;IACb;IACA,OAAO,KAAK;EACd;;EAEA;EACAiC,QAAQA,CAAA,EAAG;IACT,OAAO;MACL7E,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BC,IAAI,EAAE,IAAI,CAACA,IAAI;MACfC,MAAM,EAAE,IAAI,CAACA,MAAM;MACnBC,KAAK,EAAE,IAAI,CAACA,KAAK;MACjB2E,OAAO,EAAE,IAAI,CAAC9E,WAAW,GAAG,CAAC,GAAG,CAAC,IAAI,CAACC,IAAI,GAAG,IAAI,CAACD,WAAW,GAAG,GAAG,EAAE0D,OAAO,CAAC,CAAC,CAAC,GAAG,CAAC;MACnFqB,SAAS,EAAE,IAAI,CAACrF,MAAM,CAACsF;IACzB,CAAC;EACH;AACF;AAEA,eAAexF,IAAI","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}